<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Test-Time Domain Adaptation by Learning Domain-Aware Batch Normalization</title>
  <link rel="icon" type="image/x-icon" href="static/images/logo512.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>


<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Test-Time Domain Adaptation by Learning Domain-Aware Batch Normalization</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Yanan Wu</a><sup>1*</sup>,</span>
                <span class="author-block">
                  Zhixiang Chi</a><sup>2*</sup>,</span>
                  <span class="author-block">
                    Yang Wang</a><sup>3</sup>,</span>
					<span class="author-block">
                    Konstantinos N. Plataniotis</a><sup>2</sup>,</span>
					<span class="author-block">
                    Songhe Feng</a><sup>1</sup></span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Beijing Jiaotong University, <sup>2</sup>University of Toronto,<sup>3</sup>Concordia University
					<br>AAAI 2024</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2312.10165.pdf<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="https://arxiv.org/pdf/2312.10165.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/ynanwu/MABN" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2312.10165.pdf<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser"> -->
  <!-- <div class="container is-max-desktop"> -->
    <!-- <div class="hero-body"> -->
      <!-- <h2 class="subtitle has-text-centered"> -->
        <!-- Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus.  -->
      <!-- </h2> -->
	  <!-- <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4" -->
        <!-- type="video/mp4"> -->
      <!-- </video> -->
    <!-- </div> -->
  <!-- </div> -->
<!-- </section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<hr>
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Test-time domain adaptation aims to adapt the model trained on source domains to unseen target domains using a few unlabeled images. Emerging research has shown that the label and domain information is separately embedded in the weight matrix and batch normalization (BN) layer. Previous works normally update the whole network naively without explicitly decoupling the knowledge between label and domain. As a result, it leads to knowledge interference and defective distribution adaptation. In this work, we propose to reduce such learning interference and elevate the domain knowledge learning by only manipulating the BN layer. However, the normalization step in BN is intrinsically unstable when the statistics are re-estimated from a few samples. We find that ambiguities can be greatly reduced when only updating the two affine parameters in BN while keeping the source domain statistics. To further enhance the domain knowledge extraction from unlabeled data, we construct an auxiliary branch with label-independent self-supervised learning (SSL) to provide supervision. Moreover, we propose a bi-level optimization based on meta-learning to enforce the alignment of two learning objectives of auxiliary and main branches. The goal is to use the auxiliary branch to adapt the domain and benefit main task for subsequent inference. Our method keeps the same computational cost at inference as the auxiliary branch can be thoroughly discarded after adaptation. Extensive experiments show that our method outperforms the prior works on five WILDS real-world domain shift datasets. Our method can also be integrated with methods with label-dependent optimization to further push the performance boundary.
          </p>
        </div>
		
		
		
		
		
		
		
		<hr>
		<h2 class="title is-3">Problem Setting</h2>
		<!-- Your image here -->
		<img src="static/images/setting.png" alt="MY ALT TEXT" class="center-img"/>
		
		<br>
		<div class="content has-text-justified">
          <p>
			<br>Unsupervised Domain Adaptation (UDA) allows for the use of unlabeled data from a target domain during training, and knowledge is transferred from the data from source domains. However, it is less applicable in real-world scenarios as it requires repetitive large-scale training for every target domain.
			<br>
			<br>Domain Generalization (DG) operates under the assumption that the prior knowledge of target domains is unknown. It expects that the model trained on a source domain will perform well across all target domains. While DG is more practical, it is often less optimal since it does not adapt to the domain-specific knowledge of the target domains.
			<br>
		    <br>In this work, we focus on the problem of Test-time Domain Adaptation (TTDA) or Few-shot TTDA, which somehow combines UDA and DG. It follows the source-free setting as in DG but requires an additional learning phase at test-time for each of the target domain: when an unseen target domain is encountered at test-time, a few unlabeled images are sampled to update the model towards that domain. The adapted model is then used for testing the data in that domain (as shown in the figure above). 
          </p>
        </div>
		
		
		
		<hr>
		<h2 class="title is-3">Motivation</h2>
		<!-- Your image here -->
		<img src="static/images/motivation.png" alt="MY ALT TEXT" class="center-img"/>
		
		<br>
		<div class="content has-text-justified">
          <p>
			<br>Our work is partly inspired by the observation that the weight matrix tends to encapsulate label information, while domain-specific knowledge is embedded within the BN layer. We propose a strategic manipulation of the BN layer to optimize the acquisition and transference of domain-specific knowledge. The BN layer normalizes the input feature followed by re-scaling and shifting using two affine parameters. However, the normalization statistics computed for the target domain under TT-DA can be unstable since we only have a small batch of examples from the target domain. Instead, we propose to only adapt the two affine parameters while directly using the normalization statistics learned from source domains during training. We further use self-supervised learning method to update the affines on unlabeled target data.
			</p>
        </div>
		
		
		
		<hr>
		<h2 class="title is-3">Method Overview</h2>
		<!-- Your image here -->
		<img src="static/images/overview.png" alt="MY ALT TEXT" class="center-img"/>
		
		<br>
		<div class="content has-text-justified">
          <p>
			<br>To learn a good initialization for the affine parameters that are suitable to adapt domain-specific information, we employ two stage of learning process. In the joint training stage (a), we train the entire network to learn both label knowledge and normalization statistics by mixing all the source data and performing joint training. During the meta-auxiliary training stage (b), we first obtain the adapted parameters based on the auxiliary loss in the inner loop. Then, the meta-model is updated at the outer loop based on the main task loss computed on adapted parameters. At test-time (c), we simply apply the adaptation step to update the model specifically to an unseen target domain.
			</p>
        </div>
		
		<hr>
		<h2 class="title is-3">Experimental Results</h2>
		<!-- Your image here -->
		<img src="static/images/table_1.png" alt="MY ALT TEXT" class="center-img"/>
		
		<div class="content has-text-justified">
          <p>
			Comparison with the state-of-the-arts on the WILDS benchmark under the out-of-distribution setting. 
			</p>
        </div>
		
		
		<!-- Your image here -->
		<img src="static/images/table_2.png" alt="MY ALT TEXT" class="center-img"/>
		
		<div class="content has-text-justified">
          <p>
			Comparison with the state-of-the-arts on the DomainNet benchmark under the leave-one-out setting. 
			</p>
        </div>
		
		<!-- Your image here -->
		<img src="static/images/table_3.png" alt="MY ALT TEXT" class="center-img"/>
		
		<div class="content has-text-justified">
          <p>
			Verification of domain knowledge learning. ``No adapt" means the meta-learned \( \gamma, \beta \) are used for all domains without adaptation. ``Not matched" means each target domain randomly uses the adapted \(\tilde{\gamma}, \tilde{\beta}\) from other domains instead of its own. ``Matched" means each target domain uses its own adapted \(\tilde{\gamma}, \tilde{\beta}\).
			</p>
        </div>
		
		
		<hr>
		<img src="static/images/visual.png" alt="MY ALT TEXT" class="center-img"/>
		
		<div class="content has-text-justified">
          <p>
			t-SNE visualization of features before and after adaptation. Each data sample is represented as a point, and each color corresponds to a class randomly selected from the target domain of the iWildCam dataset.
			</p>
        </div>
		
		
		
		
		<!--End BibTex citation -->
		
		<!-- <footer class="footer"> -->
		  <!-- <div class="container"> -->
			<!-- <div class="columns is-centered"> -->
			  <!-- <div class="column is-8"> -->
				<!-- <div class="content"> -->

				  <!-- <p> -->
					<!-- Add the footer/contact -->
				  <!-- </p> -->

				<!-- </div> -->
			  <!-- </div> -->
			<!-- </div> -->
		  <!-- </div> -->
		<!-- </footer> -->
        
      </div>
    </div>
  </div>
</section>

<hr>
		<!--BibTex citation -->
		<section class="section" id="BibTeX">
		<div class="container is-max-desktop content">
		  <h2 class="title">Citation</h2>
		  <pre><code>@InProceedings{wu2023test,
		   title={Test-Time Domain Adaptation by Learning Domain-Aware Batch Normalization},
		   author={Yanan Wu, Zhixiang Chi, Yang Wang, Konstantinos N. Plataniotis, Songhe Feng},
		   booktitle={AAAI Conference on Artificial Intelligence},
		   year={2024}}</code></pre>
		</div>
		</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This website template was adapted from <a href="https://nerfies.github.io/">Nerfies website</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

  </body>
  </html>
